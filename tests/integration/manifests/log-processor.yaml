apiVersion: v1
kind: ConfigMap
metadata:
  name: processor-config
  namespace: default
  labels:
    app: log-processor
    testing.environment: github-actions
data:
  EXECUTION_MODE: "scan"
  TENANT_CONFIG_TABLE: "integration-test-tenant-configs"
  CENTRAL_LOG_DISTRIBUTION_ROLE_ARN: "arn:aws:iam::123456789012:role/CentralLogDistributionRole"
  AWS_REGION: "us-east-1"
  MAX_BATCH_SIZE: "100"
  RETRY_ATTEMPTS: "3"
  SOURCE_BUCKET: "test-logs"
  SCAN_INTERVAL: "10"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: log-processor
  namespace: default
  labels:
    app: log-processor
    testing.environment: github-actions
spec:
  replicas: 1
  selector:
    matchLabels:
      app: log-processor
  template:
    metadata:
      labels:
        app: log-processor
        testing.environment: github-actions
    spec:
      initContainers:
      - name: copy-processor
        image: python:3.13-slim
        command:
        - sh
        - -c
        - |
          # Copy the log processor script from configmap data
          cat > /app/log_processor.py << 'EOF'
          # Minimal processor for integration testing
          import json
          import time
          import os
          import sys
          import boto3
          import logging
          from datetime import datetime
          from typing import Dict, Any, List
          from botocore.exceptions import ClientError
          
          # Configure logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)
          
          # Environment variables
          TENANT_CONFIG_TABLE = os.environ.get('TENANT_CONFIG_TABLE', 'integration-test-tenant-configs')
          CENTRAL_LOG_DISTRIBUTION_ROLE_ARN = os.environ.get('CENTRAL_LOG_DISTRIBUTION_ROLE_ARN')
          AWS_REGION = os.environ.get('AWS_REGION', 'us-east-1')
          
          class NonRecoverableError(Exception):
              pass
              
          class TenantNotFoundError(NonRecoverableError):
              pass
              
          class InvalidS3NotificationError(NonRecoverableError):
              pass
          
          def extract_tenant_info_from_key(object_key: str) -> Dict[str, str]:
              """Extract tenant information from S3 object key"""
              path_parts = object_key.split('/')
              if len(path_parts) < 5:
                  raise InvalidS3NotificationError(f"Invalid object key format: {object_key}")
              
              return {
                  'cluster_id': path_parts[0],
                  'tenant_id': path_parts[1],
                  'namespace': path_parts[1], 
                  'application': path_parts[2],
                  'pod_name': path_parts[3]
              }
          
          def get_tenant_delivery_configs(tenant_id: str) -> List[Dict[str, Any]]:
              """Get tenant configurations from DynamoDB Local"""
              try:
                  # Connect to DynamoDB Local 
                  dynamodb = boto3.resource(
                      'dynamodb',
                      endpoint_url='http://dynamodb-local:8000',
                      region_name=AWS_REGION,
                      aws_access_key_id='test',
                      aws_secret_access_key='test'
                  )
                  table = dynamodb.Table(TENANT_CONFIG_TABLE)
                  
                  response = table.query(
                      KeyConditionExpression='tenant_id = :tenant_id',
                      ExpressionAttributeValues={':tenant_id': tenant_id}
                  )
                  
                  configs = response.get('Items', [])
                  if not configs:
                      raise TenantNotFoundError(f"No configs found for tenant: {tenant_id}")
                  
                  # Filter enabled configs
                  enabled_configs = [c for c in configs if c.get('enabled', True)]
                  if not enabled_configs:
                      raise TenantNotFoundError(f"No enabled configs for tenant: {tenant_id}")
                  
                  return enabled_configs
              except Exception as e:
                  logger.error(f"Failed to get tenant configs: {e}")
                  raise
          
          def should_process_application(delivery_config: Dict[str, Any], application_name: str) -> bool:
              """Check if application should be processed"""
              desired_logs = delivery_config.get('desired_logs')
              if not desired_logs:
                  return True
              return application_name.lower() in [log.lower() for log in desired_logs]
          
          def deliver_logs_to_s3(source_bucket: str, source_key: str, delivery_config: Dict[str, Any], tenant_info: Dict[str, str]) -> None:
              """Deliver logs to customer S3 bucket (MinIO)"""
              try:
                  # For integration testing, use MinIO credentials directly
                  s3_client = boto3.client(
                      's3',
                      endpoint_url='http://minio:9000',
                      aws_access_key_id='minioadmin',
                      aws_secret_access_key='minioadmin',
                      region_name='us-east-1'
                  )
                  
                  destination_bucket = delivery_config['bucket_name']
                  bucket_prefix = delivery_config.get('bucket_prefix', 'integration-logs/')
                  
                  if not bucket_prefix.endswith('/'):
                      bucket_prefix += '/'
                  
                  source_filename = source_key.split('/')[-1]
                  destination_key = (
                      f"{bucket_prefix}{tenant_info['cluster_id']}/"
                      f"{tenant_info['namespace']}/{tenant_info['application']}/"
                      f"{tenant_info['pod_name']}/{source_filename}"
                  )
                  
                  logger.info(f"Copying from s3://{source_bucket}/{source_key} to s3://{destination_bucket}/{destination_key}")
                  
                  # Perform S3-to-S3 copy
                  copy_source = {'Bucket': source_bucket, 'Key': source_key}
                  metadata = {
                      'source-bucket': source_bucket,
                      'tenant-id': tenant_info['tenant_id'],
                      'cluster-id': tenant_info['cluster_id'],
                      'application': tenant_info['application'],
                      'pod-name': tenant_info['pod_name'],
                      'delivery-timestamp': str(int(datetime.now().timestamp()))
                  }
                  
                  s3_client.copy_object(
                      Bucket=destination_bucket,
                      Key=destination_key,
                      CopySource=copy_source,
                      Metadata=metadata,
                      MetadataDirective='REPLACE'
                  )
                  
                  logger.info(f"Successfully copied to {destination_bucket}/{destination_key}")
                  
              except Exception as e:
                  logger.error(f"Failed S3 delivery: {e}")
                  raise
          
          def process_s3_object(bucket_name: str, object_key: str) -> None:
              """Process a single S3 object"""
              try:
                  tenant_info = extract_tenant_info_from_key(object_key)
                  configs = get_tenant_delivery_configs(tenant_info['tenant_id'])
                  
                  for config in configs:
                      if config['type'] == 's3':
                          if should_process_application(config, tenant_info['application']):
                              deliver_logs_to_s3(bucket_name, object_key, config, tenant_info)
                              logger.info(f"Processed S3 delivery for {tenant_info['tenant_id']}")
                          else:
                              logger.info(f"Skipped {tenant_info['application']} due to filtering")
                      # Skip CloudWatch delivery for integration testing
                      
              except Exception as e:
                  logger.error(f"Failed to process {object_key}: {e}")
          
          def main():
              """Main integration test processor"""
              logger.info("Starting integration test log processor")
              
              # Connect to MinIO and check for new files periodically
              s3_client = boto3.client(
                  's3',
                  endpoint_url='http://minio:9000',
                  aws_access_key_id='minioadmin',
                  aws_secret_access_key='minioadmin',
                  region_name='us-east-1'
              )
              
              processed_objects = set()
              
              while True:
                  try:
                      # List objects in source bucket
                      response = s3_client.list_objects_v2(Bucket='test-logs')
                      objects = response.get('Contents', [])
                      
                      for obj in objects:
                          object_key = obj['Key']
                          if object_key not in processed_objects and object_key.endswith('.json.gz'):
                              logger.info(f"Processing new object: {object_key}")
                              try:
                                  process_s3_object('test-logs', object_key)
                                  processed_objects.add(object_key)
                              except Exception as e:
                                  logger.error(f"Failed to process {object_key}: {e}")
                      
                      time.sleep(10)  # Check every 10 seconds
                      
                  except Exception as e:
                      logger.error(f"Error in main loop: {e}")
                      time.sleep(5)
          
          if __name__ == '__main__':
              main()
          EOF
          
          chmod +x /app/log_processor.py
        volumeMounts:
        - name: app-volume
          mountPath: /app
      containers:
      - name: log-processor
        image: python:3.13-slim
        command: 
        - /bin/bash
        - -c
        - |
          pip install boto3 botocore
          python3 /app/log_processor.py
        env:
        - name: EXECUTION_MODE
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: EXECUTION_MODE
        - name: TENANT_CONFIG_TABLE
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: TENANT_CONFIG_TABLE
        - name: CENTRAL_LOG_DISTRIBUTION_ROLE_ARN
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: CENTRAL_LOG_DISTRIBUTION_ROLE_ARN
        - name: AWS_REGION
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: AWS_REGION
        - name: MAX_BATCH_SIZE
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: MAX_BATCH_SIZE
        - name: RETRY_ATTEMPTS
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: RETRY_ATTEMPTS
        - name: SOURCE_BUCKET
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: SOURCE_BUCKET
        - name: SCAN_INTERVAL
          valueFrom:
            configMapKeyRef:
              name: processor-config
              key: SCAN_INTERVAL
        volumeMounts:
        - name: app-volume
          mountPath: /app
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: app-volume
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: log-processor
  namespace: default
  labels:
    app: log-processor
spec:
  selector:
    app: log-processor
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  type: ClusterIP